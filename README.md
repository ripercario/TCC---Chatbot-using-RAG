# Assistente Virtual com IA para Consulta de Documentos Corporativos

**Status do Projeto: Em Desenvolvimento (ProtÃ³tipo Funcional)**

Este repositÃ³rio contÃ©m o cÃ³digo-fonte do projeto de Trabalho de ConclusÃ£o de Curso (TCC) em Engenharia de ComputaÃ§Ã£o, que consiste em um chatbot inteligente projetado para otimizar o acesso Ã  informaÃ§Ã£o em manuais e documentos operacionais de uma empresa.

## ğŸ“– VisÃ£o Geral

Em ambientes corporativos, especialmente em setores como a logÃ­stica, a informaÃ§Ã£o crÃ­tica estÃ¡ frequentemente dispersa em extensos arquivos PDF. Isso torna a busca por procedimentos especÃ­ficos uma tarefa lenta e ineficiente.

Este projeto implementa uma soluÃ§Ã£o baseada em **RAG (Retrieval-Augmented Generation)** que permite aos colaboradores fazerem perguntas em linguagem natural e receberem respostas precisas e contextuais, extraÃ­das diretamente dos documentos da empresa.

## ğŸ—ï¸ Arquitetura

O sistema Ã© dividido em trÃªs camadas lÃ³gicas, conforme o diagrama do projeto:

1.  **Interface:** ConstruÃ­da com **Chainlit**, Ã© a camada de apresentaÃ§Ã£o responsÃ¡vel pelo fluxo de interaÃ§Ã£o com o usuÃ¡rio:
    * `Campo para entrada de pergunta` -> `Envio do prompt ao backend` -> `RecuperaÃ§Ã£o de resposta` -> `ExibiÃ§Ã£o da resposta do chatbot`.

2.  **Backend:** Onde os processos sÃ£o orquestrados. Esta camada implementa a lÃ³gica da consulta e o pipeline de dados:
    * **GeraÃ§Ã£o de Vetores:** Converte os `chunks` dos documentos em `embeddings` e os armazena em um Ã­ndice vetorial com **FAISS**.
    * **LÃ³gica de Consulta:** Recebe a pergunta, busca por `chunks` relevantes no Ã­ndice FAISS, monta um prompt final com o contexto recuperado e o envia para o modelo Llama 3 para a geraÃ§Ã£o da resposta.

3.  **Chatbot:** ContÃ©m os componentes de InteligÃªncia Artificial que sÃ£o a base da soluÃ§Ã£o:
    * **Modelo (LLM):** `Llama 3`, responsÃ¡vel por entender a pergunta e gerar a resposta final.
    * **Embedding:** `sentence-transformers/all-MiniLM-L6-v2`, responsÃ¡vel por converter os trechos de texto dos PDFs em vetores numÃ©ricos.
    * **RAG (Retrieval-Augmented Generation):** A tÃ©cnica que aumenta a capacidade do LLM com informaÃ§Ãµes de documentos externos, garantindo respostas baseadas nos manuais da empresa.

## ğŸ› ï¸ Tecnologias Utilizadas

-   **Linguagem:** Python 3.10+
-   **Interface:** Chainlit
-   **Modelo de Linguagem (LLM):** Llama 3 (via Ollama)
-   **Modelo de Embedding:** `sentence-transformers/all-MiniLM-L6-v2`
-   **Ãndice Vetorial:** FAISS
-   **ManipulaÃ§Ã£o de PDF:** PyPDFLoader
-   **Ambiente Virtual:** venv

## ğŸš€ Como Executar o Projeto

Siga os passos abaixo para configurar e rodar o projeto em seu ambiente local.

### 1. PrÃ©-requisitos

-   Python 3.10 ou superior
-   [Ollama](https://ollama.com/) instalado e em execuÃ§Ã£o.

### 2. InstalaÃ§Ã£o

**a. Clone o repositÃ³rio:**
```bash
git clone [https://github.com/ripercario/TCC---Chatbot-using-RAG.git](https://github.com/ripercario/TCC---Chatbot-using-RAG.git)
cd seu-repositorio
```

**b. Crie e ative um ambiente virtual:**
```bash
# Windows
python -m venv .venv
.\.venv\Scripts\activate

# macOS / Linux
python3 -m venv .venv
source .venv/bin/activate
```

**c. Instale as dependÃªncias:**
```bash
pip install -r requirements.txt
```

**d. Baixe o modelo Llama 3 via Ollama:**
```bash
ollama pull llama3
```

### 3. ConfiguraÃ§Ã£o

**a. Adicione os seus documentos:**
Coloque todos os manuais e documentos em formato `.pdf` dentro da pasta `/data`.

**b. Crie o Ã­ndice vetorial:**
Execute o script `create_index.py` para processar os PDFs e criar o banco de dados vetorial FAISS. Isso precisa ser feito apenas uma vez ou sempre que os documentos na pasta `/data` forem alterados.
```bash
python create_index.py
```
Ao final do processo, uma nova pasta chamada `data/faiss_index` serÃ¡ criada.

### 4. ExecuÃ§Ã£o

Com o Ollama em execuÃ§Ã£o em segundo plano, inicie a aplicaÃ§Ã£o Chainlit:
```bash
chainlit run app/app.py -w
```
A flag `-w` (watch) reinicia o servidor automaticamente sempre que vocÃª salvar uma alteraÃ§Ã£o no cÃ³digo.

Abra o seu navegador e acesse `http://localhost:8000` para comeÃ§ar a interagir com o chatbot.

## ğŸ“ Estrutura do Projeto
```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py          # LÃ³gica da interface com Chainlit
â”‚   â”œâ”€â”€ chatbot.py      # LÃ³gica de orquestraÃ§Ã£o e chamada ao modelo
â”‚   â””â”€â”€ rag_pipeline.py # FunÃ§Ãµes de criaÃ§Ã£o e consulta do Ã­ndice RAG
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ exemplo.pdf     # Coloque seus PDFs aqui
â”‚   â””â”€â”€ faiss_index/    # Criado apÃ³s a execuÃ§Ã£o de create_index.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ chainlit.md
â”œâ”€â”€ create_index.py     # Script para criar o Ã­ndice vetorial
â”œâ”€â”€ main.py             # Ponto de entrada para testes via terminal
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## âœ’ï¸ Autor

-   **Ricardo Percario de Souza Ribeiro** 